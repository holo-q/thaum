# Thaum Configuration via Environment Variables
# Copy this file to .env and fill in your values

# LLM Provider Configuration
LLM__Provider=openrouter
LLM__BaseUrl=https://openrouter.ai/api/v1
LLM__ApiKey=your-openrouter-api-key-here
LLM__DefaultModel=moonshotai/moonshot-v1-8k
LLM__Temperature=0.7
LLM__MaxTokens=4096

# OpenRouter specific settings
LLM__AppName=Thaum
LLM__SiteUrl=https://github.com/your-repo/thaum

# Alternative LLM Providers (uncomment to use)
# OpenAI
# LLM__Provider=openai
# LLM__BaseUrl=https://api.openai.com/v1
# LLM__ApiKey=sk-your-openai-api-key-here
# LLM__DefaultModel=gpt-4

# Anthropic
# LLM__Provider=anthropic
# LLM__BaseUrl=https://api.anthropic.com/v1
# LLM__ApiKey=your-anthropic-api-key-here
# LLM__DefaultModel=claude-3-sonnet-20240229

# Ollama (local)
# LLM__Provider=ollama
# LLM__BaseUrl=http://localhost:11434
# LLM__DefaultModel=llama2

# Cache Configuration
Cache__Directory=./cache
Cache__DefaultExpiration=24:00:00

# Logging Configuration
Logging__LogLevel__Default=Information
Logging__LogLevel__Thaum=Information
Logging__LogLevel__Microsoft=Warning